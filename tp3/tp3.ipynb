{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  this proces however afforded me no means of as...    EAP\n",
      "1  id17569  it never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  in his left hand was a gold snuff box from whi...    EAP\n",
      "3  id27763  how lovely is spring as we looked from windsor...    MWS\n",
      "4  id12958  finding nothing else not even gold the superin...    HPL\n",
      "5  id22965  a youth passed in solitude my best years spent...    MWS\n",
      "6  id09674  the astronomer perhaps at this point took refu...    EAP\n",
      "7  id13515         the surcingle hung in ribands from my body    EAP\n",
      "8  id19322  i knew that you could not say to yourself ster...    EAP\n",
      "9  id00912  i confess that neither the structure of langua...    MWS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spooky_cleaned.csv')\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author  \\\n",
      "0  id26305  this proces however afforded me no means of as...    EAP   \n",
      "1  id17569  it never once occurred to me that the fumbling...    HPL   \n",
      "2  id11008  in his left hand was a gold snuff box from whi...    EAP   \n",
      "3  id27763  how lovely is spring as we looked from windsor...    MWS   \n",
      "4  id12958  finding nothing else not even gold the superin...    HPL   \n",
      "\n",
      "   author_EAP  author_HPL  author_MWS  \n",
      "0         1.0         0.0         0.0  \n",
      "1         0.0         1.0         0.0  \n",
      "2         1.0         0.0         0.0  \n",
      "3         0.0         0.0         1.0  \n",
      "4         0.0         1.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "author_encoded = encoder.fit_transform(df[['author']])\n",
    "\n",
    "# Ajouter les colonnes encodées au DataFrame\n",
    "df[encoder.get_feature_names_out(['author'])] = author_encoded\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = df[\"author\"]\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = df['text'].values\n",
    "y = y_labels.values\n",
    "\n",
    "# Create stratified k-fold cross-validator\n",
    "#It ensures each fold maintains the same class distribution as the original dataset, which is crucial for imbalanced data\n",
    "skf = StratifiedKFold(n_splits=int(1/0.3), shuffle=True, random_state=0)  # n_splits=3 for ~30% test size\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    break  \n",
    "\n",
    "\n",
    "X_train = df['text'].iloc[train_index]\n",
    "X_test = df['text'].iloc[test_index]\n",
    "y_train = y_labels.iloc[train_index]\n",
    "y_test = y_labels.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce Dimensionality:\n",
    "\n",
    "    Use TfidfVectorizer with parameters like max_features to limit the number of features (e.g., max_features=5000). This reduces the encoding size and speeds up training and testing.\n",
    "    \n",
    "Use N-grams:\n",
    "\n",
    "    Incorporate n-grams (e.g., bigrams or trigrams) to capture more context. This can improve precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "count_array_tfidf = X_train_tfidf.toarray()\n",
    "df2 = pd.DataFrame(data=count_array_tfidf,columns = tfidf_vectorizer.get_feature_names_out())\n",
    "df2\n",
    "\n",
    "# Example: Assuming y_train and y_test are string labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "model1 = MLPClassifier(hidden_layer_sizes=(50, 25), activation='tanh', max_iter=500, solver='adam', random_state=1)\n",
    "\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(50, 25),activation='relu' , max_iter=200, solver='adam', alpha=0.01, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "input_dim = X_train_tfidf.shape[1]\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape before training: (13052,)\n",
      "Epoch 1/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 74ms/step - accuracy: 0.6068 - loss: 0.9748 - val_accuracy: 0.7593 - val_loss: 0.7109\n",
      "Epoch 2/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 68ms/step - accuracy: 0.9454 - loss: 0.1657 - val_accuracy: 0.8142 - val_loss: 0.4910\n",
      "Epoch 3/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 68ms/step - accuracy: 0.9876 - loss: 0.0479 - val_accuracy: 0.8145 - val_loss: 0.5838\n",
      "Epoch 4/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 68ms/step - accuracy: 0.9924 - loss: 0.0270 - val_accuracy: 0.8082 - val_loss: 0.6535\n",
      "Epoch 5/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9948 - loss: 0.0212 - val_accuracy: 0.8028 - val_loss: 0.7281\n",
      "Epoch 6/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9942 - loss: 0.0202 - val_accuracy: 0.8051 - val_loss: 0.7693\n",
      "Epoch 7/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9924 - loss: 0.0250 - val_accuracy: 0.7981 - val_loss: 0.8500\n",
      "Epoch 8/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 70ms/step - accuracy: 0.9843 - loss: 0.0437 - val_accuracy: 0.7869 - val_loss: 0.8829\n",
      "Epoch 9/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9797 - loss: 0.0573 - val_accuracy: 0.7893 - val_loss: 0.8566\n",
      "Epoch 10/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9815 - loss: 0.0498 - val_accuracy: 0.7892 - val_loss: 0.8826\n",
      "Epoch 11/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 70ms/step - accuracy: 0.9874 - loss: 0.0342 - val_accuracy: 0.7955 - val_loss: 0.8947\n",
      "Epoch 12/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9935 - loss: 0.0218 - val_accuracy: 0.8014 - val_loss: 0.9337\n",
      "Epoch 13/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9941 - loss: 0.0159 - val_accuracy: 0.7982 - val_loss: 0.9993\n",
      "Epoch 14/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 70ms/step - accuracy: 0.9946 - loss: 0.0150 - val_accuracy: 0.7991 - val_loss: 1.0260\n",
      "Epoch 15/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9940 - loss: 0.0164 - val_accuracy: 0.7915 - val_loss: 1.0694\n",
      "Epoch 16/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9933 - loss: 0.0199 - val_accuracy: 0.7906 - val_loss: 1.0653\n",
      "Epoch 17/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9910 - loss: 0.0314 - val_accuracy: 0.7909 - val_loss: 1.0949\n",
      "Epoch 18/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9917 - loss: 0.0256 - val_accuracy: 0.7939 - val_loss: 1.0691\n",
      "Epoch 19/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9928 - loss: 0.0230 - val_accuracy: 0.7932 - val_loss: 1.0871\n",
      "Epoch 20/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 70ms/step - accuracy: 0.9926 - loss: 0.0207 - val_accuracy: 0.7962 - val_loss: 1.0551\n",
      "Epoch 21/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 76ms/step - accuracy: 0.9939 - loss: 0.0186 - val_accuracy: 0.7987 - val_loss: 1.0875\n",
      "Epoch 22/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 85ms/step - accuracy: 0.9946 - loss: 0.0151 - val_accuracy: 0.7976 - val_loss: 1.0973\n",
      "Epoch 23/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 82ms/step - accuracy: 0.9931 - loss: 0.0200 - val_accuracy: 0.7968 - val_loss: 1.1179\n",
      "Epoch 24/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 81ms/step - accuracy: 0.9946 - loss: 0.0154 - val_accuracy: 0.7961 - val_loss: 1.1418\n",
      "Epoch 25/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 91ms/step - accuracy: 0.9954 - loss: 0.0120 - val_accuracy: 0.7958 - val_loss: 1.1812\n",
      "Epoch 26/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 77ms/step - accuracy: 0.9951 - loss: 0.0144 - val_accuracy: 0.7939 - val_loss: 1.2066\n",
      "Epoch 27/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 70ms/step - accuracy: 0.9960 - loss: 0.0138 - val_accuracy: 0.7899 - val_loss: 1.2652\n",
      "Epoch 28/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9937 - loss: 0.0159 - val_accuracy: 0.7860 - val_loss: 1.2411\n",
      "Epoch 29/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9940 - loss: 0.0171 - val_accuracy: 0.7850 - val_loss: 1.2501\n",
      "Epoch 30/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 70ms/step - accuracy: 0.9940 - loss: 0.0167 - val_accuracy: 0.7904 - val_loss: 1.2711\n",
      "Epoch 31/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9951 - loss: 0.0138 - val_accuracy: 0.7880 - val_loss: 1.2952\n",
      "Epoch 32/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 77ms/step - accuracy: 0.9962 - loss: 0.0118 - val_accuracy: 0.7875 - val_loss: 1.3022\n",
      "Epoch 33/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 71ms/step - accuracy: 0.9960 - loss: 0.0120 - val_accuracy: 0.7870 - val_loss: 1.3092\n",
      "Epoch 34/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9979 - loss: 0.0071 - val_accuracy: 0.7935 - val_loss: 1.3330\n",
      "Epoch 35/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9965 - loss: 0.0101 - val_accuracy: 0.7881 - val_loss: 1.3866\n",
      "Epoch 36/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9965 - loss: 0.0095 - val_accuracy: 0.7861 - val_loss: 1.3397\n",
      "Epoch 37/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9962 - loss: 0.0098 - val_accuracy: 0.7864 - val_loss: 1.3811\n",
      "Epoch 38/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9958 - loss: 0.0131 - val_accuracy: 0.7867 - val_loss: 1.3566\n",
      "Epoch 39/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9969 - loss: 0.0100 - val_accuracy: 0.7907 - val_loss: 1.3513\n",
      "Epoch 40/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 77ms/step - accuracy: 0.9958 - loss: 0.0140 - val_accuracy: 0.7909 - val_loss: 1.3283\n",
      "Epoch 41/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9967 - loss: 0.0127 - val_accuracy: 0.7884 - val_loss: 1.3795\n",
      "Epoch 42/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9974 - loss: 0.0087 - val_accuracy: 0.7893 - val_loss: 1.3872\n",
      "Epoch 43/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 70ms/step - accuracy: 0.9965 - loss: 0.0117 - val_accuracy: 0.7933 - val_loss: 1.3946\n",
      "Epoch 44/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9969 - loss: 0.0098 - val_accuracy: 0.7913 - val_loss: 1.3992\n",
      "Epoch 45/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9968 - loss: 0.0072 - val_accuracy: 0.7866 - val_loss: 1.4575\n",
      "Epoch 46/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 72ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 0.7873 - val_loss: 1.4693\n",
      "Epoch 47/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 86ms/step - accuracy: 0.9976 - loss: 0.0066 - val_accuracy: 0.7896 - val_loss: 1.5166\n",
      "Epoch 48/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 86ms/step - accuracy: 0.9963 - loss: 0.0089 - val_accuracy: 0.7867 - val_loss: 1.4859\n",
      "Epoch 49/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 70ms/step - accuracy: 0.9968 - loss: 0.0100 - val_accuracy: 0.7887 - val_loss: 1.4824\n",
      "Epoch 50/50\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - accuracy: 0.9964 - loss: 0.0086 - val_accuracy: 0.7866 - val_loss: 1.5073\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7865788264133599\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.78      0.83      0.80      2634\n",
      "         HPL       0.81      0.74      0.77      1878\n",
      "         MWS       0.78      0.78      0.78      2015\n",
      "\n",
      "    accuracy                           0.79      6527\n",
      "   macro avg       0.79      0.78      0.78      6527\n",
      "weighted avg       0.79      0.79      0.79      6527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Debugging: Check the input data shape before training\n",
    "print(\"Input data shape before training:\", X_train.shape)\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_tfidf.toarray(), y_train_one_hot, epochs=50, batch_size=32, validation_data=(X_test_tfidf.toarray(), y_test_one_hot))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predicted_labels))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predicted_labels, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "\n",
    "#save the model\n",
    "model.save('models_tp3/spooky_author_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
